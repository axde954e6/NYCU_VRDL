{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:03.979961Z",
     "iopub.status.busy": "2021-12-25T06:35:03.978793Z",
     "iopub.status.idle": "2021-12-25T06:35:06.218857Z",
     "shell.execute_reply": "2021-12-25T06:35:06.217956Z",
     "shell.execute_reply.started": "2021-12-25T06:35:03.979871Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import scipy.optimize as opt\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet50, resnet34\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.nn import init\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:07.005773Z",
     "iopub.status.busy": "2021-12-25T06:35:07.005428Z",
     "iopub.status.idle": "2021-12-25T06:35:07.04052Z",
     "shell.execute_reply": "2021-12-25T06:35:07.039475Z",
     "shell.execute_reply.started": "2021-12-25T06:35:07.005737Z"
    }
   },
   "outputs": [],
   "source": [
    "__all__ = ['xception']\n",
    "\n",
    "xception_url = 'https://www.dropbox.com/s/1hp'\n",
    "xception_url += 'lpzet9d7dv29/xception-c0a72b38.pth.tar?dl=1'\n",
    "\n",
    "model_urls = {\n",
    "    'xception': xception_url\n",
    "}\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=0, dilation=1, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride,\n",
    "                               padding, dilation,\n",
    "                               groups=in_channels, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1,\n",
    "                                   1, 0, 1, 1, bias=bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, reps,\n",
    "                 strides=1, start_with_relu=True, grow_first=True):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        if out_filters != in_filters or strides != 1:\n",
    "            self.skip = nn.Conv2d(in_filters, out_filters,\n",
    "                                  1, stride=strides, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
    "        else:\n",
    "            self.skip = None\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        rep = []\n",
    "\n",
    "        filters = in_filters\n",
    "        if grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters, out_filters,\n",
    "                                       3, stride=1, padding=1, bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "            filters = out_filters\n",
    "\n",
    "        for i in range(reps-1):\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(filters, filters, 3,\n",
    "                                       stride=1, padding=1, bias=False))\n",
    "            rep.append(nn.BatchNorm2d(filters))\n",
    "        \n",
    "        if not grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters, out_filters, 3,\n",
    "                                       stride=1, padding=1, bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "\n",
    "        if not start_with_relu:\n",
    "            rep = rep[1:]\n",
    "        else:\n",
    "            rep[0] = nn.ReLU(inplace=False)\n",
    "\n",
    "        if strides != 1:\n",
    "            rep.append(nn.MaxPool2d(3, strides, 1))\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = self.rep(inp)\n",
    "\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = inp\n",
    "\n",
    "        x += skip\n",
    "        return x\n",
    "\n",
    "\n",
    "class Xception(nn.Module):\n",
    "    \"\"\"\n",
    "    Xception optimized for the ImageNet dataset, as specified in\n",
    "    https://arxiv.org/pdf/1610.02357.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=1000):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            num_classes: number of classes\n",
    "        \"\"\"\n",
    "        super(Xception, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 2, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.block1 = Block(64, 128, 2, 2,\n",
    "                            start_with_relu=False, grow_first=True)\n",
    "        self.block2 = Block(128, 256, 2, 2,\n",
    "                            start_with_relu=True, grow_first=True)\n",
    "        self.block3 = Block(256, 728, 2, 2,\n",
    "                            start_with_relu=True, grow_first=True)\n",
    "\n",
    "        self.block4 = Block(728, 728, 3, 1,\n",
    "                            start_with_relu=True, grow_first=True)\n",
    "        self.block5 = Block(728, 728, 3, 1,\n",
    "                            start_with_relu=True, grow_first=True)\n",
    "        self.block6 = Block(728, 728, 3, 1,\n",
    "                            start_with_relu=True, grow_first=True)\n",
    "        self.block7 = Block(728, 728, 3, 1,\n",
    "                            start_with_relu=True, grow_first=True)\n",
    "\n",
    "        self.block8 = Block(728, 728, 3, 1,\n",
    "                            start_with_relu=True, grow_first=True)\n",
    "        self.block9 = Block(728, 728, 3, 1,\n",
    "                            start_with_relu=True, grow_first=True)\n",
    "        self.block10 = Block(728, 728, 3, 1,\n",
    "                             start_with_relu=True, grow_first=True)\n",
    "        self.block11 = Block(728, 728, 3, 1,\n",
    "                             start_with_relu=True, grow_first=True)\n",
    "\n",
    "        self.block12 = Block(728, 1024, 2, 2,\n",
    "                             start_with_relu=True, grow_first=False)\n",
    "\n",
    "        self.conv3 = SeparableConv2d(1024, 1536, 3, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\n",
    "\n",
    "        self.conv4 = SeparableConv2d(1536, 2048, 3, 1, 1)\n",
    "        self.bn4 = nn.BatchNorm2d(2048)\n",
    "\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.block9(x)\n",
    "        x = self.block10(x)\n",
    "        x = self.block11(x)\n",
    "        x = self.block12(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def xception(pretrained=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Construct Xception.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Xception(**kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['xception']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:07.610849Z",
     "iopub.status.busy": "2021-12-25T06:35:07.610527Z",
     "iopub.status.idle": "2021-12-25T06:35:07.615001Z",
     "shell.execute_reply": "2021-12-25T06:35:07.613785Z",
     "shell.execute_reply.started": "2021-12-25T06:35:07.610817Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN = '../input/human-protein-atlas-image-classification/train/'\n",
    "TEST = '../input/human-protein-atlas-image-classification/test/'\n",
    "LABELS = '../input/human-protein-atlas-image-classification/train.csv'\n",
    "SUBMIT = '../input/human-protein-atlas-image-classification/'\n",
    "SUBMIT += 'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:08.314475Z",
     "iopub.status.busy": "2021-12-25T06:35:08.314126Z",
     "iopub.status.idle": "2021-12-25T06:35:08.322339Z",
     "shell.execute_reply": "2021-12-25T06:35:08.321475Z",
     "shell.execute_reply.started": "2021-12-25T06:35:08.314442Z"
    }
   },
   "outputs": [],
   "source": [
    "name_label_dict = {\n",
    "    0: 'Nucleoplasm',\n",
    "    1: 'Nuclear membrane',\n",
    "    2: 'Nucleoli',   \n",
    "    3: 'Nucleoli fibrillar center',\n",
    "    4: 'Nuclear speckles',\n",
    "    5: 'Nuclear bodies',\n",
    "    6: 'Endoplasmic reticulum',   \n",
    "    7: 'Golgi apparatus',\n",
    "    8: 'Peroxisomes',\n",
    "    9: 'Endosomes',\n",
    "    10: 'Lysosomes',\n",
    "    11: 'Intermediate filaments',\n",
    "    12: 'Actin filaments',\n",
    "    13: 'Focal adhesion sites',\n",
    "    14: 'Microtubules',\n",
    "    15: 'Microtubule ends',\n",
    "    16: 'Cytokinetic bridge',\n",
    "    17: 'Mitotic spindle',\n",
    "    18: 'Microtubule organizing center',\n",
    "    19: 'Centrosome',\n",
    "    20: 'Lipid droplets',\n",
    "    21: 'Plasma membrane',\n",
    "    22: 'Cell junctions',\n",
    "    23: 'Mitochondria',\n",
    "    24: 'Aggresome',\n",
    "    25: 'Cytosol',\n",
    "    26: 'Cytoplasmic bodies',\n",
    "    27: 'Rods & rings'\n",
    "}\n",
    "\n",
    "N_CLASSES = len(name_label_dict)\n",
    "print('The number of classes: {}'.format(N_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:08.898524Z",
     "iopub.status.busy": "2021-12-25T06:35:08.89804Z",
     "iopub.status.idle": "2021-12-25T06:35:08.971268Z",
     "shell.execute_reply": "2021-12-25T06:35:08.97047Z",
     "shell.execute_reply.started": "2021-12-25T06:35:08.898476Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(LABELS)\n",
    "sub_df = pd.read_csv(SUBMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:10.059448Z",
     "iopub.status.busy": "2021-12-25T06:35:10.059098Z",
     "iopub.status.idle": "2021-12-25T06:35:10.45343Z",
     "shell.execute_reply": "2021-12-25T06:35:10.452654Z",
     "shell.execute_reply.started": "2021-12-25T06:35:10.059409Z"
    }
   },
   "outputs": [],
   "source": [
    "cls_counts = Counter(cls for classes in df['Target'].str.split() for cls in classes)\n",
    "counts_x = [i[1] for i in cls_counts.most_common(N_CLASSES)]\n",
    "counts_y = [name_label_dict[int(i[0])] for i in cls_counts.most_common(N_CLASSES)]\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.barplot(y=counts_y, x=counts_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:11.675737Z",
     "iopub.status.busy": "2021-12-25T06:35:11.675342Z",
     "iopub.status.idle": "2021-12-25T06:35:12.402595Z",
     "shell.execute_reply": "2021-12-25T06:35:12.401735Z",
     "shell.execute_reply.started": "2021-12-25T06:35:11.675701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fold\n",
    "n_folds = 10\n",
    "fold_cls_counts = defaultdict(int)\n",
    "folds = [-1] * len(df)\n",
    "for item in tqdm(df.sample(frac=1, random_state=42).itertuples(),total=len(df)):\n",
    "    cls = min(item.Target.split(), key=lambda cls: cls_counts[cls])\n",
    "    fold_counts = [(f, fold_cls_counts[f, cls]) for f in range(n_folds)]\n",
    "    min_count = min([count for _, count in fold_counts])\n",
    "    random.seed(item.Index)\n",
    "    fold = random.choice([f for f, count in fold_counts if count == min_count])\n",
    "    folds[item.Index] = fold\n",
    "    for cls in item.Target.split():\n",
    "        fold_cls_counts[fold, cls] += 1\n",
    "df['fold'] = folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:13.242492Z",
     "iopub.status.busy": "2021-12-25T06:35:13.242123Z",
     "iopub.status.idle": "2021-12-25T06:35:13.267927Z",
     "shell.execute_reply": "2021-12-25T06:35:13.267097Z",
     "shell.execute_reply.started": "2021-12-25T06:35:13.242456Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_idx = 0\n",
    "train_df = df[df['fold']!=valid_idx][['Id', 'Target']].reset_index(drop=True)\n",
    "valid_df = df[df['fold']==valid_idx][['Id', 'Target']].reset_index(drop=True)\n",
    "print('There are {} samples in the training set.'.format(len(train_df)))\n",
    "print('There are {} samples in the validation set.'.format(len(valid_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:14.195881Z",
     "iopub.status.busy": "2021-12-25T06:35:14.195371Z",
     "iopub.status.idle": "2021-12-25T06:35:14.20323Z",
     "shell.execute_reply": "2021-12-25T06:35:14.20234Z",
     "shell.execute_reply.started": "2021-12-25T06:35:14.195842Z"
    }
   },
   "outputs": [],
   "source": [
    "def open_rgby(path, id, size=None): # RGBY image\n",
    "    colors = ['red','green','blue','yellow']\n",
    "    flags = cv2.IMREAD_GRAYSCALE\n",
    "    if size is None:\n",
    "        img = [cv2.imread(os.path.join(path, id+'_'+color+'.png'), flags).astype(np.float32)/255 \n",
    "               for color in colors]\n",
    "    else:\n",
    "        img = []\n",
    "        for color in colors:\n",
    "            src_img = cv2.imread(os.path.join(path, id+'_'+color+'.png'), flags)\n",
    "            tar_img = cv2.resize(src_img, (2*size, size), interpolation=cv2.INTER_CUBIC).astype(np.float32)/255\n",
    "            img.append(tar_img)\n",
    "\n",
    "    return np.stack(img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:15.027852Z",
     "iopub.status.busy": "2021-12-25T06:35:15.027427Z",
     "iopub.status.idle": "2021-12-25T06:35:15.040033Z",
     "shell.execute_reply": "2021-12-25T06:35:15.037784Z",
     "shell.execute_reply.started": "2021-12-25T06:35:15.027811Z"
    }
   },
   "outputs": [],
   "source": [
    "class AtlasDataset(Dataset):\n",
    "    def __init__(self, df, path, size=None, label=True):        \n",
    "        self.df = df.copy()\n",
    "        self.path = path\n",
    "        self.size = size\n",
    "        self.label = label\n",
    "        if self.label:\n",
    "            self.df['Target'] = [[int(i) for i in s.split()] for s in self.df['Target']] \n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        img = open_rgby(self.path, self.df['Id'].iloc[index], self.size)\n",
    "        if self.label:\n",
    "            target = np.eye(N_CLASSES,dtype=np.float)[self.df['Target'].iloc[index]].sum(axis=0) \n",
    "        else:\n",
    "            target = np.zeros(N_CLASSES,dtype=np.int)\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:15.69143Z",
     "iopub.status.busy": "2021-12-25T06:35:15.69106Z",
     "iopub.status.idle": "2021-12-25T06:35:15.697822Z",
     "shell.execute_reply": "2021-12-25T06:35:15.696895Z",
     "shell.execute_reply.started": "2021-12-25T06:35:15.691372Z"
    }
   },
   "outputs": [],
   "source": [
    "size= 512\n",
    "bs = 10 # batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:16.586465Z",
     "iopub.status.busy": "2021-12-25T06:35:16.586054Z",
     "iopub.status.idle": "2021-12-25T06:35:16.732629Z",
     "shell.execute_reply": "2021-12-25T06:35:16.731836Z",
     "shell.execute_reply.started": "2021-12-25T06:35:16.586419Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(AtlasDataset(train_df, TRAIN, size), batch_size=bs, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(AtlasDataset(valid_df, TRAIN, size), batch_size=bs, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:17.234038Z",
     "iopub.status.busy": "2021-12-25T06:35:17.233717Z",
     "iopub.status.idle": "2021-12-25T06:35:17.240211Z",
     "shell.execute_reply": "2021-12-25T06:35:17.239122Z",
     "shell.execute_reply.started": "2021-12-25T06:35:17.234005Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loader  = DataLoader(AtlasDataset(sub_df, TEST, size, False), batch_size=bs, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:20.244821Z",
     "iopub.status.busy": "2021-12-25T06:35:20.244476Z",
     "iopub.status.idle": "2021-12-25T06:35:20.257073Z",
     "shell.execute_reply": "2021-12-25T06:35:20.255941Z",
     "shell.execute_reply.started": "2021-12-25T06:35:20.244791Z"
    }
   },
   "outputs": [],
   "source": [
    "class AvgPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.squeeze(F.avg_pool2d(x, x.shape[2:]))\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        encoder = resnet50(pretrained=False)\n",
    "        if pretrained:\n",
    "            path=\"../input/pytorch-pretrained-models/resnet50-19c8e357.pth\"\n",
    "            encoder.load_state_dict(torch.load(path))\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False) \n",
    "        \n",
    "        if (pretrained):\n",
    "            w = encoder.conv1.weight\n",
    "            self.conv1.weight = nn.Parameter(torch.cat((w,0.5*(w[:,:1,:,:]+w[:,2:,:,:])),dim=1)) \n",
    "        \n",
    "        self.bn1 = encoder.bn1\n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = nn.Sequential(self.conv1,self.relu,self.bn1,self.maxpool)\n",
    "        self.layer1 = encoder.layer1\n",
    "        self.layer2 = encoder.layer2\n",
    "        self.layer3 = encoder.layer3\n",
    "        self.layer4 = encoder.layer4\n",
    "        self.avgpool = AvgPool()\n",
    "        self.fc = nn.Sequential(nn.Dropout(p=0.5), nn.Linear(encoder.fc.in_features, num_classes)) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:21.197193Z",
     "iopub.status.busy": "2021-12-25T06:35:21.196802Z",
     "iopub.status.idle": "2021-12-25T06:35:21.213026Z",
     "shell.execute_reply": "2021-12-25T06:35:21.212016Z",
     "shell.execute_reply.started": "2021-12-25T06:35:21.19716Z"
    }
   },
   "outputs": [],
   "source": [
    "class myXception(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            num_classes: number of classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        encoder=xception(pretrained=pretrained)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, 3,2, 0, bias=False)\n",
    "        self.bn1 = encoder.bn1\n",
    "        self.relu =encoder.relu\n",
    "\n",
    "        self.conv2 = encoder.conv2\n",
    "        self.bn2 = encoder.bn2\n",
    "        #do relu here\n",
    "\n",
    "        self.block1=encoder.block1\n",
    "        self.block2=encoder.block2\n",
    "        self.block3=encoder.block3\n",
    "\n",
    "        self.block4=encoder.block4\n",
    "        self.block5=encoder.block5\n",
    "        self.block6=encoder.block6\n",
    "        self.block7=encoder.block7\n",
    "\n",
    "        self.block8=encoder.block8\n",
    "        self.block9=encoder.block9\n",
    "        self.block10=encoder.block10\n",
    "        self.block11=encoder.block11\n",
    "\n",
    "        self.block12=encoder.block12\n",
    "\n",
    "        self.conv3 = encoder.conv3\n",
    "        self.bn3 = encoder.bn3\n",
    "\n",
    "        #do relu here\n",
    "        self.conv4 = encoder.conv4\n",
    "        self.bn4 = encoder.bn4\n",
    "\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.block9(x)\n",
    "        x = self.block10(x)\n",
    "        x = self.block11(x)\n",
    "        x = self.block12(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:21.682666Z",
     "iopub.status.busy": "2021-12-25T06:35:21.682216Z",
     "iopub.status.idle": "2021-12-25T06:35:26.795765Z",
     "shell.execute_reply": "2021-12-25T06:35:26.794093Z",
     "shell.execute_reply.started": "2021-12-25T06:35:21.682627Z"
    }
   },
   "outputs": [],
   "source": [
    "#model = ResNet50(num_classes=N_CLASSES)\n",
    "model = myXception(num_classes=N_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:27.189414Z",
     "iopub.status.busy": "2021-12-25T06:35:27.188952Z",
     "iopub.status.idle": "2021-12-25T06:35:27.203213Z",
     "shell.execute_reply": "2021-12-25T06:35:27.202089Z",
     "shell.execute_reply.started": "2021-12-25T06:35:27.189352Z"
    }
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        target = target.float()\n",
    "        max_val = (-logit).clamp(min=0)\n",
    "        loss = logit - logit * target + max_val + \\\n",
    "               ((-max_val).exp() + (-logit - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-logit * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        if len(loss.size())==2:\n",
    "            loss = loss.sum(dim=1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training\n",
    "\n",
    "## prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:28.326185Z",
     "iopub.status.busy": "2021-12-25T06:35:28.325837Z",
     "iopub.status.idle": "2021-12-25T06:35:28.396741Z",
     "shell.execute_reply": "2021-12-25T06:35:28.395506Z",
     "shell.execute_reply.started": "2021-12-25T06:35:28.326141Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "os.makedirs('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:29.473429Z",
     "iopub.status.busy": "2021-12-25T06:35:29.473102Z",
     "iopub.status.idle": "2021-12-25T06:35:35.63483Z",
     "shell.execute_reply": "2021-12-25T06:35:35.6339Z",
     "shell.execute_reply.started": "2021-12-25T06:35:29.473379Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"../input/xception/xception_400_th40_acc39.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:35.636898Z",
     "iopub.status.busy": "2021-12-25T06:35:35.636524Z",
     "iopub.status.idle": "2021-12-25T06:35:35.684322Z",
     "shell.execute_reply": "2021-12-25T06:35:35.683565Z",
     "shell.execute_reply.started": "2021-12-25T06:35:35.636858Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = FocalLoss()\n",
    "model = model.to(device)\n",
    "\n",
    "lr = 0.0002\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:35.686415Z",
     "iopub.status.busy": "2021-12-25T06:35:35.686027Z",
     "iopub.status.idle": "2021-12-25T06:35:35.694298Z",
     "shell.execute_reply": "2021-12-25T06:35:35.692311Z",
     "shell.execute_reply.started": "2021-12-25T06:35:35.686364Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(epoch, history=None):\n",
    "    model.train() \n",
    "    t = tqdm(train_loader)\n",
    "    \n",
    "    for batch_idx, (img_batch, label_batch) in enumerate(t):\n",
    "        img_batch = img_batch.to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(img_batch)\n",
    "        loss = criterion(output, label_batch)\n",
    "        t.set_description(f'train_loss (l={loss:.4f})')\n",
    "        \n",
    "        if history is not None:\n",
    "            history.loc[epoch + batch_idx / len(train_loader), 'train_loss'] = loss.data.cpu().numpy()\n",
    "        \n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "    \n",
    "    torch.save(model.state_dict(), 'models/epoch{}.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:35.996441Z",
     "iopub.status.busy": "2021-12-25T06:35:35.996149Z",
     "iopub.status.idle": "2021-12-25T06:35:36.003287Z",
     "shell.execute_reply": "2021-12-25T06:35:36.00236Z",
     "shell.execute_reply.started": "2021-12-25T06:35:35.996406Z"
    }
   },
   "outputs": [],
   "source": [
    "def binarize_prediction(probabilities, threshold: float, argsorted=None,\n",
    "                        min_labels=1, max_labels=10):\n",
    "    \"\"\" Return matrix of 0/1 predictions, same shape as probabilities.\n",
    "    \"\"\"\n",
    "    assert probabilities.shape[1] == N_CLASSES\n",
    "    if argsorted is None:\n",
    "        argsorted = probabilities.argsort(axis=1)\n",
    "    max_mask = _make_mask(argsorted, max_labels)\n",
    "    min_mask = _make_mask(argsorted, min_labels)\n",
    "    prob_mask = probabilities > threshold\n",
    "    return (max_mask & prob_mask) | min_mask\n",
    "\n",
    "def _make_mask(argsorted, top_n: int):\n",
    "    mask = np.zeros_like(argsorted, dtype=np.uint8)\n",
    "    col_indices = argsorted[:, -top_n:].reshape(-1)\n",
    "    row_indices = [i // top_n for i in range(len(col_indices))]\n",
    "    mask[row_indices, col_indices] = 1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:37.02943Z",
     "iopub.status.busy": "2021-12-25T06:35:37.029Z",
     "iopub.status.idle": "2021-12-25T06:35:37.048699Z",
     "shell.execute_reply": "2021-12-25T06:35:37.04772Z",
     "shell.execute_reply.started": "2021-12-25T06:35:37.029372Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(epoch, history=None): \n",
    "    model.eval()\n",
    "    valid_loss = 0.\n",
    "    all_predictions, all_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (img_batch, label_batch) in enumerate(valid_loader):\n",
    "            all_targets.append(label_batch.numpy().copy())\n",
    "            img_batch = img_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            output = model(img_batch)\n",
    "            loss = criterion(output, label_batch)\n",
    "            valid_loss += loss.data\n",
    "            predictions = torch.sigmoid(output)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    valid_loss /= (batch_idx+1)\n",
    "    \n",
    "    if history is not None:\n",
    "        history.loc[epoch, 'valid_loss'] = valid_loss.cpu().numpy()\n",
    "    \n",
    "    print('Epoch: {}\\tLR: {:.6f}\\tValid Loss: {:.4f}'.format(\n",
    "        epoch, optimizer.state_dict()['param_groups'][0]['lr'], valid_loss))\n",
    "    \n",
    "    def get_score(y_pred):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore', category=UndefinedMetricWarning)\n",
    "            return f1_score(all_targets, y_pred, average='macro')\n",
    "    \n",
    "    metrics = {}\n",
    "    argsorted = all_predictions.argsort(axis=1)\n",
    "    for threshold in [0.05,0.10,0.15,0.2,0.25,0.3,0.35,0.4,0.45]: \n",
    "        metrics[threshold] = get_score(\n",
    "            binarize_prediction(all_predictions, threshold, argsorted))\n",
    "    best_thr = max(metrics, key=metrics.get)\n",
    "    print(' | '.join(f'thr_{k:.2f} {v:.3f}' for k, v in sorted(\n",
    "        metrics.items(), key=lambda kv: -kv[1])[:5]))\n",
    "    \n",
    "    return valid_loss, best_thr, metrics[best_thr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T06:35:38.387918Z",
     "iopub.status.busy": "2021-12-25T06:35:38.387591Z",
     "iopub.status.idle": "2021-12-25T09:41:58.580572Z",
     "shell.execute_reply": "2021-12-25T09:41:58.578601Z",
     "shell.execute_reply.started": "2021-12-25T06:35:38.387888Z"
    }
   },
   "outputs": [],
   "source": [
    "history_train = pd.DataFrame()\n",
    "history_valid = pd.DataFrame()\n",
    "\n",
    "n_epochs = 100\n",
    "init_epoch = 0\n",
    "max_lr_changes = 0\n",
    "valid_losses = []\n",
    "threshold = {}\n",
    "macro_f1 = {}\n",
    "lr_reset_epoch = init_epoch\n",
    "patience = 2\n",
    "lr_changes = 1\n",
    "best_valid_loss = 1000.\n",
    "\n",
    "for epoch in range(init_epoch, n_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    train_model(epoch, history_train)\n",
    "    valid_loss, best_thr, best_f1 = evaluate(epoch, history_valid)\n",
    "    valid_losses.append(valid_loss)\n",
    "    threshold[epoch] = best_thr\n",
    "    macro_f1[epoch] = best_f1\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    elif (patience and epoch - lr_reset_epoch > patience and\n",
    "          min(valid_losses[-patience:]) > best_valid_loss):\n",
    "        lr_changes +=1\n",
    "        if lr_changes > max_lr_changes: \n",
    "            break\n",
    "        lr /= 5 \n",
    "        print(f'lr updated to {lr}')\n",
    "        lr_reset_epoch = epoch\n",
    "        optimizer.param_groups[0]['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T09:56:28.139865Z",
     "iopub.status.busy": "2021-12-25T09:56:28.139508Z",
     "iopub.status.idle": "2021-12-25T09:56:28.477519Z",
     "shell.execute_reply": "2021-12-25T09:56:28.47664Z",
     "shell.execute_reply.started": "2021-12-25T09:56:28.13983Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "ax[0].plot(history_train['train_loss'].iloc[100:])\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Train Loss')\n",
    "\n",
    "ax[1].plot(history_valid['valid_loss'])\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Valid Loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T09:56:33.785428Z",
     "iopub.status.busy": "2021-12-25T09:56:33.785089Z",
     "iopub.status.idle": "2021-12-25T09:56:33.9402Z",
     "shell.execute_reply": "2021-12-25T09:56:33.939128Z",
     "shell.execute_reply.started": "2021-12-25T09:56:33.785377Z"
    }
   },
   "outputs": [],
   "source": [
    "best_epoch = max(macro_f1, key=macro_f1.get)\n",
    "print('The best epoch is epoch {}'.format(best_epoch))\n",
    "\n",
    "#model.load_state_dict(torch.load(\"../input/xception/epoch6.pth\"))\n",
    "model.load_state_dict(torch.load('models/epoch{}.pth'.format(best_epoch)))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T09:56:35.849416Z",
     "iopub.status.busy": "2021-12-25T09:56:35.84905Z",
     "iopub.status.idle": "2021-12-25T10:04:56.482435Z",
     "shell.execute_reply": "2021-12-25T10:04:56.481442Z",
     "shell.execute_reply.started": "2021-12-25T09:56:35.849367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "outputlist = []\n",
    "for img_batch, _ in tqdm(test_loader):\n",
    "    with torch.no_grad():\n",
    "        output = torch.sigmoid(model(img_batch.to(device)))\n",
    "    output = output.data.cpu().numpy()\n",
    "    for i in output: \n",
    "        outputlist.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-25T10:09:03.685076Z",
     "iopub.status.busy": "2021-12-25T10:09:03.684742Z",
     "iopub.status.idle": "2021-12-25T10:09:04.031419Z",
     "shell.execute_reply": "2021-12-25T10:09:04.030655Z",
     "shell.execute_reply.started": "2021-12-25T10:09:03.685045Z"
    }
   },
   "outputs": [],
   "source": [
    "thr = 0.40\n",
    "print('The best threshold is {:.3f}'.format(thr))\n",
    "\n",
    "prediction = [' '.join([str(i) for i in np.argwhere((j > thr).astype(int)==1).reshape(-1)]) for j in outputlist]\n",
    "sub_df['Predicted'] = prediction\n",
    "sub_df.to_csv('submission_512_xception.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
